<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k317h</title>
    <link>https://keithzeng.github.io/</link>
    <description>Recent content on k317h</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Feb 2019 18:50:42 -0800</lastBuildDate>
    
	<atom:link href="https://keithzeng.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sequence Alignment Revisited</title>
      <link>https://keithzeng.github.io/posts/sequence-alignment-revisited/</link>
      <pubDate>Sat, 09 Feb 2019 18:50:42 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/sequence-alignment-revisited/</guid>
      <description>Problem1 Sequence Alignment
We can reduce the space by using only two column instead.
Space-Efficient-Alignment(X ,Y)
 Array B[0&amp;hellip;m,0&amp;hellip;1] Initialize B[i,0]=iδ for each i (just as in column 0 of A) For j=1,&amp;hellip;,n  B[0,1]=j$\delta$ (since this corresponds to entry A[0,j]) For i=1,&amp;hellip;,m  B[i, 1]= min[$\alpha x_iy_j$ + B[i − 1, 0], $\delta$ + B[i−1,1], $\delta$ + B[i,0]]  Endfor Move column 1 of B to column 0 to make room for next iteration:  Update B[i, 0]= B[i, 1] for each i   Endfor  However, this doesn&amp;rsquo;t left enough information to get back the information about alignment.</description>
    </item>
    
    <item>
      <title>Sequence Alignment</title>
      <link>https://keithzeng.github.io/posts/sequence-alignment/</link>
      <pubDate>Sat, 09 Feb 2019 17:56:45 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/sequence-alignment/</guid>
      <description>Problem1 When you type &amp;ldquo;ocurrance&amp;rdquo;, the brower will will prompt “Perhaps you mean occurrence?”
How does the search engine knows?
How does it know the most similar word?
How do we determine the similarity between words?
The definition of similarity will be based on the optimal alignment between X and Y where X = $x_1x_2 &amp;hellip; x_m$, Y = $y_1y_2 &amp;hellip; y_n$.
 when there is gap (character not matched), we pay $\delta$ when we match character p and q, we pay $\alpha_{pq}$, where $\alpha_{pq}$ = 0 goal is to minimize the sum of cost.</description>
    </item>
    
    <item>
      <title>RNA Secondary Structure</title>
      <link>https://keithzeng.github.io/posts/rna-secondary-structure/</link>
      <pubDate>Sat, 09 Feb 2019 17:07:21 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/rna-secondary-structure/</guid>
      <description>Problem1 We have bases of {A,C,G,T} in DNA sequence, where A-T and C-G form a pair. A sinlge strand of RNA will loop back, resulting shape called secondary structure.
Let $B = b_1b_2 &amp;hellip; b_n$, where $b_i$ = {A,C,G,U} also A-U, C-G form pair
Then the Secondary structure, S ={(i, j)}
 no sharp turn, i &amp;lt; j - 4 pair is either {A,U}, {C,G} (either order) no base appear in more than 1 matching no crossing condition, (i,j) and (k, l) $\in$ S, we can&amp;rsquo;t have i &amp;lt; k &amp;lt; j &amp;lt; l  Algorithm Following the same analysis</description>
    </item>
    
    <item>
      <title>Knapsack</title>
      <link>https://keithzeng.github.io/posts/knapsack/</link>
      <pubDate>Sat, 09 Feb 2019 14:09:38 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/knapsack/</guid>
      <description>Problem1 Each request has value $v_i$ and weight $w_i$, and we have constraint that the total of request weight $\leq$ W.
Greedy doesn&amp;rsquo;t work because sorting the W either in decreasing or increasing manner don&amp;rsquo;t produce the optimal solution.
 {W/2 + 1, W/2, W/2} {1, W/2, W/2}  Using dynamic programing, we can use reduce this problem to whether or not each request belongs to the optimal solution O again.</description>
    </item>
    
    <item>
      <title>Segmented Least Square</title>
      <link>https://keithzeng.github.io/posts/segmented-least-square/</link>
      <pubDate>Sat, 09 Feb 2019 12:54:29 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/segmented-least-square/</guid>
      <description>Problem1 Data $P = (x_1,y_1), (x_2, y_2)&amp;hellip;(x_n, y_n)$ for $x_1 &amp;lt; x_2 &amp;lt; &amp;hellip; &amp;lt; x_n$.
Given a line y=ax+b, and error
$$Error(L, P) = \sum_{i=1}^n(y_i - ax_i -b)^2$$
And it has closed form solution of
$$a = \frac{n\sum_i x_iy_i - (\sum_i x_i)(\sum_i y_i)}{n\sum_i x_i^2 - (\sum_i x_i)^2}$$
and
$$b = \frac {\sum_i y_i - a \sum_i x_i}{n}$$
However, if the data is show as below (maybe fitted with two lines), we can&amp;rsquo;t just use the above formula.</description>
    </item>
    
    <item>
      <title>Weighted Interval Scheduling</title>
      <link>https://keithzeng.github.io/posts/weighted-interval-scheduling/</link>
      <pubDate>Fri, 08 Feb 2019 23:22:27 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/weighted-interval-scheduling/</guid>
      <description>Problem1 Instead of just fitting as many tasks like regular interval scheduling problem, we have weight associates with the interval. Now we want to maximize the value.
So basically, $f_i$ finish time sorted in non-decreasing order, $v_i$ is the value, and $s_i$ is the start time. We want $S \subseteq {1&amp;hellip; n}$.
We also define $p(j)$ which is the largest i &amp;lt; j which is disjoint(compatible) with j.
Algorithm Idea:</description>
    </item>
    
    <item>
      <title>Gradient Descent</title>
      <link>https://keithzeng.github.io/posts/gradient-descent/</link>
      <pubDate>Thu, 07 Feb 2019 22:21:16 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/gradient-descent/</guid>
      <description>Gradient Descent A simple algorithm to go &amp;ldquo;downward&amp;rdquo; against the gradient of the function. Algebrically: $$w_{t+1} = w_t - \eta \nabla f(w_t)$$ where $\eta$ is called learning rate or step size.
Step Size  $\eta$ too small, slow convergence $\eta$ too large, solution will bounce around  In practice:
 Set $\eta$ to be a smalle constant Backtracking line search (work when $\nabla f$ is continuous)  Parameter $\bar{\alpha}, c \in (0,1), \rho \in (0,1)$.</description>
    </item>
    
    <item>
      <title>Convex Optimization</title>
      <link>https://keithzeng.github.io/posts/convex-optimization/</link>
      <pubDate>Thu, 07 Feb 2019 22:15:35 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/convex-optimization/</guid>
      <description>Gradient &amp;amp;&amp;amp; Hessian The gradient of a f, d x 1, can be represented as follow $$ \nabla f(x) = \begin{bmatrix} \frac {\partial f(x)} {\partial x_1} \newline &amp;hellip;\newline \frac {\partial f(x)} {\partial x_d} \end{bmatrix} $$
and the Hessian, d x d, can be represented as
$$ \nabla^2 f(x) = \begin{bmatrix} \frac {\partial^2 f(x)} {\partial x_1^2} &amp;amp; \frac {\partial^2 f(x)} {\partial x_1x_2} &amp;amp; \frac {\partial^2 f(x)} {\partial x_1x_d} \newline &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip;\newline \frac {\partial^2 f(x)} {\partial x_d^2} &amp;amp; \frac {\partial^2 f(x)} {\partial x_dx_2} &amp;amp; \frac {\partial^2 f(x)} {\partial x_dx_d} \newline \end{bmatrix} $$</description>
    </item>
    
    <item>
      <title>Positive Semi Definite</title>
      <link>https://keithzeng.github.io/posts/positive-semi-definite/</link>
      <pubDate>Thu, 07 Feb 2019 15:29:45 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/positive-semi-definite/</guid>
      <description>PSD differential equation
Any matrix $A_{d \times d}$ is said to be PSD if $x^TAx \geq$ 0 $\forall$ vector $x_{d \times 1}$.
Examples Identity matrix $I_{d \times d}$: $$ A = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \newline 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} $$
Then
\begin{align} x^TAx &amp;amp;= \sum_{i = 1}^d \sum_{i = 1}^d A_{i,j}x_ix_j \newline &amp;amp;= \sum_{i = 1}^d A_{i,i} x_i^2\newline &amp;amp;= \sum_{i = 1}^d x_i^2 \newline &amp;amp;\geq 0 \end{align}</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://keithzeng.github.io/posts/logistic-regression/</link>
      <pubDate>Fri, 01 Feb 2019 14:39:49 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/logistic-regression/</guid>
      <description>Uncertainty in Prediction Related to Linear Regression.
The available features x do not contain enough information to perfectly predict y, such as
 x = medical record for patients at risk for a disease y = will he contact disease in next 5 years  Model We still going to use linear model for conditional probability estmation
$$w_1x_1 + w_2x_2 + &amp;hellip; + w_dx_d + b = w \cdot x + b$$</description>
    </item>
    
    <item>
      <title>Matrix</title>
      <link>https://keithzeng.github.io/posts/matrix/</link>
      <pubDate>Fri, 01 Feb 2019 02:06:44 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/matrix/</guid>
      <description>Determinant Calculation Determinant of 2x2 matrix
$$ A= \begin{bmatrix} a &amp;amp; b \newline c &amp;amp; d \end{bmatrix} $$
$|A| = det(A) = ad -bc$
Determinant of 3x3 matrix, also called expansion of the determinant by first row. Link.
$$ B= \begin{bmatrix} a &amp;amp; b &amp;amp; c \newline d &amp;amp; e &amp;amp; f \newline g &amp;amp; h &amp;amp; k \end{bmatrix} $$
$|B| = det(B) = a\begin{vmatrix} e &amp;amp; f \newline h &amp;amp; k \end{vmatrix} -b\begin{vmatrix} d &amp;amp; f \newline g &amp;amp; k \end{vmatrix} +c\begin{vmatrix} d &amp;amp; e \newline g &amp;amp; h \end{vmatrix}$</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://keithzeng.github.io/posts/linear-regression/</link>
      <pubDate>Thu, 31 Jan 2019 22:30:23 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/linear-regression/</guid>
      <description>Basic Idea Fit a line to a bunch of points.
Example Without extra information, we will predict the mean 2.47.
Average squared error = $\mathbb{E} [(studentGPA - predictedGPA)^2]$ = Variance
If we have SAT scores, then we can fit a line.
Now if we predict based on this line, the MSE drops to 0.43.
This is a regression problem with:
 Predictor variable: SAT score Response variable: College GPA  Formula For $\mathbb{R}$ $$y = ax + b$$</description>
    </item>
    
    <item>
      <title>Bayes Optimal Classifier</title>
      <link>https://keithzeng.github.io/posts/bayes-optimal-classifier/</link>
      <pubDate>Thu, 31 Jan 2019 18:00:14 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/bayes-optimal-classifier/</guid>
      <description>Background Marginal Distribution
Three ways to sample from P
 Draw (x,y) Draw y according to its marginal distribution, then x according to the conditional distribution of x | y Draw X according to its marginal distribution, then Y according to the conditional distribution of y | x  Define:
$\mu$: distribution on $X$
$\eta$: conditional distribution y|x
Classifier Normal Classifier
$h : x \rightarrow y$
$R(h) = Pr_{(x,y) \in p} (h(x) \neq y)$, where R = risk</description>
    </item>
    
    <item>
      <title>Marginal Distribution</title>
      <link>https://keithzeng.github.io/posts/marginal-distribution/</link>
      <pubDate>Thu, 31 Jan 2019 17:57:34 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/marginal-distribution/</guid>
      <description>Marginal Distribution: It&amp;rsquo;s a function that gives the probability based on only subset of the variables.1. For example,
Or in mathematical way, for discrete
$$Pr(X = x) = \sum_y Pr(X = x, Y = y) = \sum_y Pr(X = x | Y = y) Pr(Y = y)$$
and for continuous
$$p_X(x) = \int_y p_{X,Y}(x,y) dy = \int_y p_{X|Y}(x|y)p_Y(y) dy$$
and it can also be written as Expected Vaue
$$p_X(x) = \int_y p_{X|Y} (x|y) p_Y(y) dy = \mathbb{E}_Y[p_{X|Y} (x|y)]$$</description>
    </item>
    
    <item>
      <title>lp norm</title>
      <link>https://keithzeng.github.io/posts/lp-norm/</link>
      <pubDate>Thu, 31 Jan 2019 17:02:35 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/lp-norm/</guid>
      <description>Families of Distance Function $l_p$ norm The most common one is $l_2$ norm (Euclidean distance):
$$||x - z||_2 = \sqrt{\sum_{i=1}^{m}(x_i - z_i)^2}$$
Notes: sometime 2 is dropped.
For $p \geq 1$, the $l_p$ distance:
$$||x - z||_p = (\sum_{i=1}^{m}(x_i - z_i)^p)^{1/p}$$
Special case:
$l_1$ distance: $$||x - z||_1 = \sum_{i=1}^{m}|x_i - z_i|$$
$l_\infty$ distance:
$$||x - z||_1 = max_i |x_i - z_i|$$
Metric space Let $X$ be the space that data lie.</description>
    </item>
    
    <item>
      <title>Nearest Neighbor Classification</title>
      <link>https://keithzeng.github.io/posts/nearest-neighbor-classification/</link>
      <pubDate>Thu, 31 Jan 2019 16:08:27 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/nearest-neighbor-classification/</guid>
      <description>Nearest Neighbor Classification Procedures  Assemble a data set (training set)  How to classify a new image x?  find its closest neighbor y, and label it the same   Notes:
 training set of 60000 images test set of 10000 images  How do we determine if two data (images) are closest? With 28 x 28 image, we can strech it to become a vector of 784.</description>
    </item>
    
    <item>
      <title>Fast Fourtier Transform</title>
      <link>https://keithzeng.github.io/posts/fast-fourtier-transform/</link>
      <pubDate>Wed, 30 Jan 2019 00:31:46 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/fast-fourtier-transform/</guid>
      <description>Problem Given two vectors $a = (a_1, a_2, a_{n-1})$ and $b = (a_1, b_2, b_{n-1})$.
The convolution of a * b is a vector with 2n - 1 coordinates, where coordinate k is $\sum_{(i,j):i+j=k|i,j &amp;lt; n} a_ib_j$, which is can be written as
$$a ∗ b = (a_0b_0, a_0b_1 + a_1b_0, a_0b_2 + a_1b_1 + a_2b_0, &amp;hellip; , a_{n−2}b_{n−1} + a_{n−1}b_{n−2}, a_{n−1}b_{n−1})$$
Or an n x n table, whose (i, j) entry is $a_ib_j$</description>
    </item>
    
    <item>
      <title>Integer Multiplication</title>
      <link>https://keithzeng.github.io/posts/integer-multiplication/</link>
      <pubDate>Wed, 30 Jan 2019 00:02:09 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/integer-multiplication/</guid>
      <description>Problem Wiki Explanation
Algorithm Let&amp;rsquo;s say we define $x = x_1 2^{n/2} + x_0$, then xy become \begin{align} xy &amp;amp;= (x_1 2^{n/2} + x_0)(y_1 2^{n/2} + y_0) \newline &amp;amp;= x_1 y_1 2^n + (x_1 y_0 + x_0 y_1)2^{n/2} + x_0 y_0 \end{align}
So we have $$T(n) \leq 4T(n/2) + cn$$ But this is essentially $$T(n) \leq O(n^{\log_2 q}) = O(n^2)$$
However, we can reduce the time by observing that $(x_1 + x_0)(y_1 + y_0) = x_1y_1 + x_1y_0 + x_0y_1 + x_0y_0$.</description>
    </item>
    
    <item>
      <title>Closest Point</title>
      <link>https://keithzeng.github.io/posts/closest-point/</link>
      <pubDate>Tue, 29 Jan 2019 23:36:54 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/closest-point/</guid>
      <description>Problem Given n points in the plane, find the pair that is closest together.
Brute force takes $O(n^2)$.
Algorithm Let&amp;rsquo;s $d(p_i, p_j)$ = Euclidean distance.
In 1-d, we can simply sort points and compute the distance with the next point, we then have complexity of O(nlogn). In 2-d, we can&amp;rsquo;t applied the same thing.
We will use divide and conquer. We find the closest pair in the left and closest pair in the right, and hoping to get it in linear time.</description>
    </item>
    
    <item>
      <title>Counting Inversion</title>
      <link>https://keithzeng.github.io/posts/counting-inversion/</link>
      <pubDate>Tue, 29 Jan 2019 22:37:45 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/counting-inversion/</guid>
      <description>Problem Application in ranking, also called corraborative filtering.
Comparing two rankings and decide how similar they are, or how many pairs are out of order.
To quantify this, we count the number of inversions. The inversion is defined as two indices i &amp;lt; j that $a_i &amp;gt; a_j$. Algorithm Brute-Force $T(n) = O(n^2)$
Modified Merge-Sort By leverage the merge process form merger-sort, we can count the number of inversion. Basically, when the element from A is appended, there is not inversion.</description>
    </item>
    
    <item>
      <title>Merge Sort</title>
      <link>https://keithzeng.github.io/posts/merge-sort/</link>
      <pubDate>Tue, 29 Jan 2019 01:14:32 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/merge-sort/</guid>
      <description>Problem Sort the elements
Abstract the behavior:
1. Divide the input into two pieces of equal size O(n) 1. solve the two subproblems on these pieces separately by recursion 1. combine the two results into an overall solution O(n)
Recurrence Time Complexity q = 2 T(n) ≤ 2T(n/2) + cn
To analyze the above recurrence relation, check below image.  At level j, we have $2^j$ nodes with size $n/2^j$ Each node takes $cn/2^j$, so level j takes $cn/2^j$ x $2^j = cn$ There are logn levels, so T(n) = O(nlogn)  General Case For q &amp;gt; 2 T(n) ≤ qT(n/2) + cn</description>
    </item>
    
    <item>
      <title>Margin of Error</title>
      <link>https://keithzeng.github.io/posts/margin-error/</link>
      <pubDate>Sun, 27 Jan 2019 22:33:16 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/margin-error/</guid>
      <description>Z-Score vs T-Score Z-Score Link
Z-Score&amp;rsquo;s formula $$z = \frac{X - \mu}{\sigma}$$ where X = sample mean, $\mu$ = population means, $\sigma$ = population standard deviation.
Also, we use Z Score when sample size &amp;gt;= 30 or we know the population&amp;rsquo;s mean dna SD.
Z Table
T-Score T-Score T-Score&amp;rsquo;s formula $$T = \frac{X - \mu}{s/ \sqrt{n}}$$ where X = sample mean, $\mu$ = population mean, s = sample standard deviation, and n = sample size.</description>
    </item>
    
    <item>
      <title>Prototype Selection</title>
      <link>https://keithzeng.github.io/posts/prototype-selection/</link>
      <pubDate>Sun, 27 Jan 2019 20:25:22 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/prototype-selection/</guid>
      <description>Backgrond kNN prototype selection Summary List
There are couple drawbacks for KNN
 high storage for data computation for decision boundary intolerance to noise  There are couple methods address above issue
 better similarity metric or better distance function k-d trees or R-trees as storage reduction technique (prototype selection)  Prototype Selection 1. edition method - remove noise 1. condensation method - remove superfluous dataset 1. hybrid method - achive elimination of noise and superfluous at the same time</description>
    </item>
    
    <item>
      <title>CSE250 Proj1</title>
      <link>https://keithzeng.github.io/posts/cse250-proj1/</link>
      <pubDate>Sun, 27 Jan 2019 16:22:07 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/cse250-proj1/</guid>
      <description>Project 1
1. Prototype Selection Divide data into ${S_1, S_2, &amp;hellip;, S_c}$, where c = number of labels. For each dataset $S_i$, I select M/c points as new training set for that label, which are the centroids by running k-means clustering algorithm.
My idea was to capture as many distinct types in for each label and meanwhile reduce the noise by selecting only the center of the each type.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://keithzeng.github.io/about/</link>
      <pubDate>Sun, 27 Jan 2019 03:17:07 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Archives</title>
      <link>https://keithzeng.github.io/archives/</link>
      <pubDate>Sun, 27 Jan 2019 02:04:15 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/archives/</guid>
      <description></description>
    </item>
    
    <item>
      <title>K Clustering</title>
      <link>https://keithzeng.github.io/posts/k-clustering/</link>
      <pubDate>Fri, 25 Jan 2019 21:51:25 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/k-clustering/</guid>
      <description>Problem We have set of objects $U = \{o_1, o_2, &amp;hellip;\}$, and we want to split them into k clusters.
We also have following definition for distance function.
 $\forall_{i,j} dist(p_i, p_j) = dist(p_j, p_j)$ $\forall_{i,j} dist(p_i, p_i) = 0$ $\forall_{i,j} dist(p_i, p_j) &amp;gt; 0$.  At the end, we should have $C = \{C_1, C_2, &amp;hellip; C_K\}$.
Let&amp;rsquo;s define spacing to be the minimum dist between clusters. Our goal is to find the k-clustering with maximum spacing.</description>
    </item>
    
    <item>
      <title>Probability</title>
      <link>https://keithzeng.github.io/posts/probability/</link>
      <pubDate>Thu, 24 Jan 2019 22:27:30 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/probability/</guid>
      <description>Discrete Random Variables A random variable is a number whose value depends upon the outcome of a random experiement. Such as tossing a coin 10 times and let X be the number of Head.
A discrete random variable X has finitely countable values $x_i = 1, 2&amp;hellip;$ and $p(x_i) = P(X = x_i)$ is called probability mass function.
Probability mass functions has following properties:
 For all i, $p(x_i) &amp;gt; 0$ For any interval $P(X \in B) = \sum_{x_i \in B}p(x_i)$ $\sum_{i}p(x_i) = 1$  There are many types of discrete random variable</description>
    </item>
    
    <item>
      <title>My First Post</title>
      <link>https://keithzeng.github.io/posts/my-first-post/</link>
      <pubDate>Thu, 24 Jan 2019 16:32:01 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/my-first-post/</guid>
      <description>Hello, this is the first post</description>
    </item>
    
  </channel>
</rss>