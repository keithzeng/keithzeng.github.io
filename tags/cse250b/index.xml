<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cse250b on k317h</title>
    <link>https://keithzeng.github.io/tags/cse250b/</link>
    <description>Recent content in cse250b on k317h</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Feb 2019 10:37:54 -0800</lastBuildDate>
    
	<atom:link href="https://keithzeng.github.io/tags/cse250b/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CSE250 Proj2</title>
      <link>https://keithzeng.github.io/hws/cse250-proj2/</link>
      <pubDate>Mon, 18 Feb 2019 10:37:54 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/hws/cse250-proj2/</guid>
      <description>Coordinate Descent (a) In this project I compared different methods using different approaches. Besides the random coordinate descent (RCD), I implemented two ways to choose the coordinate: cyclic coordinate descent method(CCD) and greatest coordinate descent method (GCD). CCD updates the coordinate in cycle and GCD update the coordinate with greatest derivative.1
(b) For every iteration, the coordinate, i, being picked will be updated with below methods.
Method 1 (Minimize Loss Function)</description>
    </item>
    
    <item>
      <title>CSE250 Proj1</title>
      <link>https://keithzeng.github.io/hws/cse250-proj1/</link>
      <pubDate>Sun, 27 Jan 2019 16:22:07 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/hws/cse250-proj1/</guid>
      <description>Project 1
1. Prototype Selection Divide data into ${S_1, S_2, &amp;hellip;, S_c}$, where c = number of labels. For each dataset $S_i$, I select M/c points as new training set for that label, which are the centroids by running k-means clustering algorithm.
My idea was to capture as many distinct types in for each label and meanwhile reduce the noise by selecting only the center of the each type.</description>
    </item>
    
    <item>
      <title>Probability</title>
      <link>https://keithzeng.github.io/posts/probability/</link>
      <pubDate>Thu, 24 Jan 2019 22:27:30 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/probability/</guid>
      <description>Discrete Random Variables A random variable is a number whose value depends upon the outcome of a random experiement. Such as tossing a coin 10 times and let X be the number of Head.
A discrete random variable X has finitely countable values $x_i = 1, 2&amp;hellip;$ and $p(x_i) = P(X = x_i)$ is called probability mass function.
Probability mass functions has following properties:
 For all i, $p(x_i) &amp;gt; 0$ For any interval $P(X \in B) = \sum_{x_i \in B}p(x_i)$ $\sum_{i}p(x_i) = 1$  There are many types of discrete random variable</description>
    </item>
    
  </channel>
</rss>