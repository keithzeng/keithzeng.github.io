<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta name="generator" content="Hugo 0.53" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Statistical Learning | k317h</title>
    <meta property="og:title" content="Statistical Learning - k317h">
    <meta property="og:type" content="article">
        
        
    <meta name="Keywords" content="Java,Web,Software">
    <meta name="description" content="Statistical Learning">
        
    <meta name="author" content="k317h">
    <meta property="og:url" content="https://keithzeng.github.io/series/statistical-learning/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href="/css/normalize.css">
    
    <link rel="stylesheet" href="/css/style.css">
    <link rel="alternate" type="application/rss+xml+xml" href="https://keithzeng.github.io/series/statistical-learning/index.xml" title="k317h" />
    <script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>

    


    
    
</head>


<body>
<header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://keithzeng.github.io">
                        k317h
                    </a>
                
                <p class="description">A Blog to Record My Progress...</p>
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://keithzeng.github.io">Main</a>
                    
                    <a  href="https://keithzeng.github.io/books/" title="Books">Books</a>
                    
                    <a  href="https://keithzeng.github.io/archives/" title="Archives">Archives</a>
                    
                    <a  href="https://keithzeng.github.io/about/" title="About">About</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>


<div id="body">
    <div class="container">
        <div class="col-group">

            <div class="col-8" id="main">
                <div class="res-cons">
                    <h3 class="archive-title">
                        Articles in
                        <span class="keyword">statistical-learning</span>
                        series
                    </h3>

                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/logistic-regression/">Logistic Regression</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/2/1
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/machine-learning">machine learning</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Uncertainty in Prediction Related to Linear Regression.
The available features x do not contain enough information to perfectly predict y, such as
 x = medical record for patients at risk for a disease y = will he contact disease in next 5 years  Model We still going to use linear model for conditional probability estmation
$$w_1x_1 + w_2x_2 + &hellip; + w_dx_d + b = w \cdot x + b$$……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/logistic-regression/">Read More</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/linear-regression/">Linear Regression</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/1/31
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/machine-learning">machine learning</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Basic Idea Fit a line to a bunch of points.
Example Without extra information, we will predict the mean 2.47.
Average squared error = $\mathbb{E} [(studentGPA - predictedGPA)^2]$ = Variance
If we have SAT scores, then we can fit a line.
Now if we predict based on this line, the MSE drops to 0.43.
This is a regression problem with:
 Predictor variable: SAT score Response variable: College GPA  Formula For $\mathbb{R}$ $$y = ax + b$$……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/linear-regression/">Read More</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/bayes-optimal-classifier/">Bayes Optimal Classifier</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/1/31
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/machine-learning">machine learning</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Background Marginal Distribution
Three ways to sample from P
 Draw (x,y) Draw y according to its marginal distribution, then x according to the conditional distribution of x | y Draw X according to its marginal distribution, then Y according to the conditional distribution of y | x  Define:
$\mu$: distribution on $X$
$\eta$: conditional distribution y|x
Classifier Normal Classifier
$h : x \rightarrow y$
$R(h) = Pr_{(x,y) \in p} (h(x) \neq y)$, where R = risk……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/bayes-optimal-classifier/">Read More</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/lp-norm/">lp norm</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/1/31
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/machine-learning">machine learning</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Families of Distance Function $l_p$ norm The most common one is $l_2$ norm (Euclidean distance):
$$||x - z||_2 = \sqrt{\sum_{i=1}^{m}(x_i - z_i)^2}$$
Notes: sometime 2 is dropped.
For $p \geq 1$, the $l_p$ distance:
$$||x - z||_p = (\sum_{i=1}^{m}(x_i - z_i)^p)^{1/p}$$
Special case:
$l_1$ distance: $$||x - z||_1 = \sum_{i=1}^{m}|x_i - z_i|$$
$l_\infty$ distance:
$$||x - z||_1 = max_i |x_i - z_i|$$
Metric space Let $X$ be the space that data lie.……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/lp-norm/">Read More</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/nearest-neighbor-classification/">Nearest Neighbor Classification</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/1/31
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/machine-learning">machine learning</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Nearest Neighbor Classification Procedures  Assemble a data set (training set)  How to classify a new image x?  find its closest neighbor y, and label it the same   Notes:
 training set of 60000 images test set of 10000 images  How do we determine if two data (images) are closest? With 28 x 28 image, we can strech it to become a vector of 784.……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/nearest-neighbor-classification/">Read More</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/margin-error/">Margin of Error</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/1/27
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/mathematic">mathematic</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Z-Score vs T-Score Z-Score Link
Z-Score&rsquo;s formula $$z = \frac{X - \mu}{\sigma}$$ where X = sample mean, $\mu$ = population means, $\sigma$ = population standard deviation.
Also, we use Z Score when sample size &gt;= 30 or we know the population&rsquo;s mean dna SD.
Z Table
T-Score T-Score T-Score&rsquo;s formula $$T = \frac{X - \mu}{s/ \sqrt{n}}$$ where X = sample mean, $\mu$ = population mean, s = sample standard deviation, and n = sample size.……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/margin-error/">Read More</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/prototype-selection/">Prototype Selection</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/1/27
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/machine-learning">machine learning</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Backgrond kNN prototype selection Summary List
There are couple drawbacks for KNN
 high storage for data computation for decision boundary intolerance to noise  There are couple methods address above issue
 better similarity metric or better distance function k-d trees or R-trees as storage reduction technique (prototype selection)  Prototype Selection 1. edition method - remove noise 1. condensation method - remove superfluous dataset 1. hybrid method - achive elimination of noise and superfluous at the same time……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/prototype-selection/">Read More</a></p>
                            </div>
                        </article>
                    
                        <article class="post">
                            <header>
                                <h1 class="post-title">
                                    <a href="https://keithzeng.github.io/posts/probability/">Probability</a>
                                </h1>
                            </header>
                            <date class="post-meta meta-date">
                                2019/1/24
                            </date>
                            
                            <div class="post-meta meta-category">
                                |
                                
                                    <a href="https://keithzeng.github.io/categories/mathematic">mathematic</a>
                                
                            </div>
                            
                            <div class="post-content">
                                Discrete Random Variables A random variable is a number whose value depends upon the outcome of a random experiement. Such as tossing a coin 10 times and let X be the number of Head.
A discrete random variable X has finitely countable values $x_i = 1, 2&hellip;$ and $p(x_i) = P(X = x_i)$ is called probability mass function.
Probability mass functions has following properties:
 For all i, $p(x_i) &gt; 0$ For any interval $P(X \in B) = \sum_{x_i \in B}p(x_i)$ $\sum_{i}p(x_i) = 1$  There are many types of discrete random variable……
                                <p class="readmore"><a href="https://keithzeng.github.io/posts/probability/">Read More</a></p>
                            </div>
                        </article>
                    

                    




                </div>
            </div>

            <div id="secondary">
    <section class="widget">
        <form id="search" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://keithzeng.github.io">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">Most Recents</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://keithzeng.github.io/posts/matrix/" title="Matrix">Matrix</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/logistic-regression/" title="Logistic Regression">Logistic Regression</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/linear-regression/" title="Linear Regression">Linear Regression</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/bayes-optimal-classifier/" title="Bayes Optimal Classifier">Bayes Optimal Classifier</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/marginal-distribution/" title="Marginal Distribution">Marginal Distribution</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/lp-norm/" title="lp norm">lp norm</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/nearest-neighbor-classification/" title="Nearest Neighbor Classification">Nearest Neighbor Classification</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/fast-fourtier-transform/" title="Fast Fourtier Transform">Fast Fourtier Transform</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/integer-multiplication/" title="Integer Multiplication">Integer Multiplication</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/posts/closest-point/" title="Closest Point">Closest Point</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">Categories</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://keithzeng.github.io/categories/algorithm/">algorithm(6)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/categories/machine-learning/">machine-learning(6)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/categories/mathematic/">mathematic(4)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/categories/other/">other(3)</a>
    </li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title">Series</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://keithzeng.github.io/series/divide-and-conquer/">divide-and-conquer(5)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/series/greedy/">greedy(1)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/series/matrix/">matrix(1)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/series/other/">other(3)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/series/probability/">probability(1)</a>
    </li>
    
    <li>
        <a href="https://keithzeng.github.io/series/statistical-learning/">statistical-learning(8)</a>
    </li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title">Tags</h3>
<div class="tagcloud">
    
    <a href="https://keithzeng.github.io/tags/bayes/">bayes</a>
    
    <a href="https://keithzeng.github.io/tags/confidence-interval/">confidence-interval</a>
    
    <a href="https://keithzeng.github.io/tags/cse202/">cse202</a>
    
    <a href="https://keithzeng.github.io/tags/cse250b/">cse250b</a>
    
    <a href="https://keithzeng.github.io/tags/determinant/">determinant</a>
    
    <a href="https://keithzeng.github.io/tags/distance/">distance</a>
    
    <a href="https://keithzeng.github.io/tags/distribution/">distribution</a>
    
    <a href="https://keithzeng.github.io/tags/knn/">knn</a>
    
    <a href="https://keithzeng.github.io/tags/logistic/">logistic</a>
    
    <a href="https://keithzeng.github.io/tags/lpnorm/">lpnorm</a>
    
    <a href="https://keithzeng.github.io/tags/metric/">metric</a>
    
    <a href="https://keithzeng.github.io/tags/mnist/">mnist</a>
    
    <a href="https://keithzeng.github.io/tags/notes/">notes</a>
    
    <a href="https://keithzeng.github.io/tags/other/">other</a>
    
    <a href="https://keithzeng.github.io/tags/probability/">probability</a>
    
    <a href="https://keithzeng.github.io/tags/rank/">rank</a>
    
    <a href="https://keithzeng.github.io/tags/regression/">regression</a>
    
    <a href="https://keithzeng.github.io/tags/statistic/">statistic</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">Other</h3>
        <ul class="widget-list">
            <li><a href="https://keithzeng.github.io/index.xml">RSS</a></li>
        </ul>
    </section>
</div>
        </div>
    </div>
</div>
<footer id="footer">
    <div class="container">
        &copy; 2019 <a href="https://keithzeng.github.io">k317h By k317h</a>.
    </div>
</footer>

<script type="text/javascript">
    
    (function(){
        $("pre code").parent().addClass("line-numbers")
    }())

    window.MathJax = {
        tex2jax: {
            inlineMath: [ ['$','$'] ],
            processEscapes: true
        }
    };
</script>
<script type="text/javascript" src="/js/prism.js" async="true"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src="/js/totop.js?v=0.0.0" async=""></script>



<script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>





</body>
</html>
