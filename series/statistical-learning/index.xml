<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Learning on k317h</title>
    <link>https://keithzeng.github.io/series/statistical-learning/</link>
    <description>Recent content in Statistical Learning on k317h</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Jan 2019 17:02:35 -0800</lastBuildDate>
    
	<atom:link href="https://keithzeng.github.io/series/statistical-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>lp norm</title>
      <link>https://keithzeng.github.io/posts/lp-norm/</link>
      <pubDate>Thu, 31 Jan 2019 17:02:35 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/lp-norm/</guid>
      <description>Families of Distance Function $l_p$ norm The most common one is $l_2$ norm (Euclidean distance):
$$||x - z||_2 = \sqrt{\sum_{i=1}^{m}(x_i - z_i)^2}$$
Notes: sometime 2 is dropped.
For $p \geq 1$, the $l_p$ distance:
$$||x - z||_p = (\sum_{i=1}^{m}(x_i - z_i)^p)^{1/p}$$
Special case:
$l_1$ distance: $$||x - z||_1 = \sum_{i=1}^{m}|x_i - z_i|$$
$l_\infty$ distance:
$$||x - z||_1 = max_i |x_i - z_i|$$
Metric space Let $X$ be the space that data lie.</description>
    </item>
    
    <item>
      <title>Nearest Neighbor Classification</title>
      <link>https://keithzeng.github.io/posts/nearest-neighbor-classification/</link>
      <pubDate>Thu, 31 Jan 2019 16:08:27 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/nearest-neighbor-classification/</guid>
      <description>Nearest Neighbor Classification Procedures  Assemble a data set (training set)  How to classify a new image x?  find its closest neighbor y, and label it the same   Notes:
 training set of 60000 images test set of 10000 images  How do we determine if two data (images) are closest? With 28 x 28 image, we can strech it to become a vector of 784.</description>
    </item>
    
    <item>
      <title>Margin of Error</title>
      <link>https://keithzeng.github.io/posts/margin-error/</link>
      <pubDate>Sun, 27 Jan 2019 22:33:16 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/margin-error/</guid>
      <description>Z-Score vs T-Score Z-Score Link
Z-Score&amp;rsquo;s formula $$z = \frac{X - \mu}{\sigma}$$ where X = sample mean, $\mu$ = population means, $\sigma$ = population standard deviation.
Also, we use Z Score when sample size &amp;gt;= 30 or we know the population&amp;rsquo;s mean dna SD.
Z Table
T-Score T-Score T-Score&amp;rsquo;s formula $$T = \frac{X - \mu}{s/ \sqrt{n}}$$ where X = sample mean, $\mu$ = population mean, s = sample standard deviation, and n = sample size.</description>
    </item>
    
    <item>
      <title>Prototype Selection</title>
      <link>https://keithzeng.github.io/posts/prototype-selection/</link>
      <pubDate>Sun, 27 Jan 2019 20:25:22 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/prototype-selection/</guid>
      <description>Backgrond kNN prototype selection Summary List
There are couple drawbacks for KNN
 high storage for data computation for decision boundary intolerance to noise  There are couple methods address above issue
 better similarity metric or better distance function k-d trees or R-trees as storage reduction technique (prototype selection)  Prototype Selection 1. edition method - remove noise 1. condensation method - remove superfluous dataset 1. hybrid method - achive elimination of noise and superfluous at the same time</description>
    </item>
    
    <item>
      <title>Probability</title>
      <link>https://keithzeng.github.io/posts/probability/</link>
      <pubDate>Thu, 24 Jan 2019 22:27:30 -0800</pubDate>
      
      <guid>https://keithzeng.github.io/posts/probability/</guid>
      <description>Discrete Random Variables A random variable is a number whose value depends upon the outcome of a random experiement. Such as tossing a coin 10 times and let X be the number of Head.
A discrete random variable X has finitely countable values $x_i = 1, 2&amp;hellip;$ and $p(x_i) = P(X = x_i)$ is called probability mass function.
Probability mass functions has following properties:
 For all i, $p(x_i) &amp;gt; 0$ For any interval $P(X \in B) = \sum_{x_i \in B}p(x_i)$ $\sum_{i}p(x_i) = 1$  There are many types of discrete random variable</description>
    </item>
    
  </channel>
</rss>